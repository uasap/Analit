{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOVLpV47GpN/F5G0G5kUyqg",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/uasap/Analit/blob/main/%D0%9A%D0%BE%D0%BF%D0%B8%D1%8F_%D0%B1%D0%BB%D0%BE%D0%BA%D0%BD%D0%BE%D1%82%D0%B0_%22Untitled11_ipynb%22.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "будем работать только с частью нашего набора данных, разбитой на 4 категории из 20 возможных:\n",
        "\n",
        "список файлов, совпадающих с нужными категориями\n"
      ],
      "metadata": {
        "id": "HvkV6khQ7D2A"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6LZWQ_-WsV23",
        "outputId": "21c02fe1-9527-4804-c78f-a26fb27b8878"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "source": [
        "categories = ['alt.atheism', 'soc.religion.christian',  'comp.graphics', 'sci.med']\n",
        "\n",
        "from sklearn.datasets import fetch_20newsgroups\n",
        "twenty_train = fetch_20newsgroups(subset='train',\n",
        "    categories=categories, shuffle=True, random_state=42)\n",
        "\n",
        "twenty_train.target_names\n",
        "['alt.atheism', 'comp.graphics', 'sci.med', 'soc.religion.christian']"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Возвращаемый набор данных — это scikit-learn совокупность: одномерный контейнер с полями, которые могут интерпретироваться как ключи в словаре python (dict keys), проще говоря — как признаки объекта (object attributes). Например, target_names содержит список названий запрошенных категорий:\n",
        "\n",
        "выведем на печать первые строки из первого загруженного файла:"
      ],
      "metadata": {
        "id": "gc5v9iD48q5l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"\\n\".join(twenty_train.data[0].split(\"\\n\")[:3]))\n",
        "print(twenty_train.target_names[twenty_train.target[0]])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "66Je0ZOJsxj9",
        "outputId": "de7da5bf-7c1e-4fbd-aba5-596e57ab1059"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "From: sd345@city.ac.uk (Michael Collier)\n",
            "Subject: Converting images to HP LaserJet III?\n",
            "Nntp-Posting-Host: hampton\n",
            "comp.graphics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Алгоритмы для обучения с учителем (контролируемого обучения) требуют, чтобы у каждого документа в обучающей выборке была помета определенной категории. В нашем случае, категория — это название новостной выборки"
      ],
      "metadata": {
        "id": "qgUDXpqP9UVU"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        " название категории:"
      ],
      "metadata": {
        "id": "7gjMrHIY9b2u"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "twenty_train.target[:10]\n",
        "for t in twenty_train.target[:10]:\n",
        "     print(twenty_train.target_names[t])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SL751xactK-D",
        "outputId": "7704e5da-569b-4552-c928-0836e55903c4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "comp.graphics\n",
            "comp.graphics\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "soc.religion.christian\n",
            "sci.med\n",
            "sci.med\n",
            "sci.med\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Извлечение характерных признаков из текстовых файлов\n",
        "\n",
        "Чтобы использовать машинное обучение на текстовых документах, первым делом, нужно перевести текстовое содержимое в числовой вектор признаков.\n",
        "«Мешок слов» (набор слов)\n",
        "\n",
        "Наиболее интуитивно понятный способ сделать описанное выше преобразование — это представить текст в виде набора слов:\n",
        "приписать уникальный целочисленный индекс каждому слову, появляющемуся в документах в обучающей выборке (например, построив словарь из слов с целочисленными индексами).\n",
        "для каждого документа #i посчитать количество употреблений каждого слова w и сохранить его (количество) в X[i, j]. Это будет значение признака #j, где j — это индекс слова w в словаре.\n",
        "\n",
        "Предобработка текста, токенизация и отфильтровывание стоп-слов включены в состав высоко уровневого компонента, который позволяет создать словарь характерных признаков и перевести документы в векторы признаков:"
      ],
      "metadata": {
        "id": "7jiPoXvP99yL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "count_vect = CountVectorizer()\n",
        "X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
        "X_train_counts.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D4Oy5k_jtuUE",
        "outputId": "10546762-1958-496e-d3c6-efe4146ec15f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CountVectorizer поддерживает подсчет N-грам слов или последовательностей символов. Векторизатор строит словарь индексов признаков:"
      ],
      "metadata": {
        "id": "v-QJlOvY_kPK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "count_vect.vocabulary_.get(u'algorithm')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T_TvUZvruOgq",
        "outputId": "48aa50aa-f618-4fee-985c-0111b39355f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4690"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "в длинных документах среднее количество словоупотреблений будет выше, чем в коротких, даже если они посвящены одной теме.\n",
        "Чтобы избежать этих потенциальных несоответствий, достаточно разделить количество употреблений каждого слова в документе на общее количество слов в документе. Этот новый признак называется tf — Частота термина.\n",
        "Следующее уточнение меры tf — это снижение веса слова, которое появляется во многих документах в корпусе, и отсюда является менее информативным, чем те, которые используются только в небольшой части корпуса. Примером низко ифнормативных слов могут служить служебные слова, артикли, предлоги, союзы и т.п.\n",
        "Это снижение называется tf–idf, что значит “Term Frequency times Inverse Document Frequency” (обратная частота термина)."
      ],
      "metadata": {
        "id": "0G1Cg2as_xmw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import TfidfTransformer\n",
        "tf_transformer = TfidfTransformer(use_idf=False).fit(X_train_counts)\n",
        "X_train_tf = tf_transformer.transform(X_train_counts)\n",
        "X_train_tf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6K2Yblh9uWsy",
        "outputId": "25c1b3d4-66b2-4d0b-f31c-91fdf9d754bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf_transformer = TfidfTransformer()\n",
        "X_train_tfidf = tfidf_transformer.fit_transform(X_train_counts)\n",
        "X_train_tfidf.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Ha74dCEueBy",
        "outputId": "f0e1f171-f53b-484a-8ba4-2ebd6a7fe9d4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(2257, 35788)"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "обучим классификатор предсказывать категорию текста.  начнем с Наивного Байесовского классификатора. scikit-learn включает в себя несколько вариантов этого классификатора. Самый подходящий для подсчета слов — это его поли номинальный вариант:"
      ],
      "metadata": {
        "id": "nP1_lufoACIY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.naive_bayes import MultinomialNB\n",
        "clf = MultinomialNB().fit(X_train_tfidf, twenty_train.target)"
      ],
      "metadata": {
        "id": "R2knwFzOuu4K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "нужно извлечь фичи (характерные признаки), используя почти такую же последовательность как и ранее. Разница состоит в том, используется метод transform вместо fit_transform из transformers, так как они уже были применены к нашей обучающей выборке:"
      ],
      "metadata": {
        "id": "vrWjbuR0AR2A"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "docs_new = ['God is love', 'OpenGL on the GPU is fast']\n",
        "X_new_counts = count_vect.transform(docs_new)\n",
        "X_new_tfidf = tfidf_transformer.transform(X_new_counts)\n",
        "\n",
        "predicted = clf.predict(X_new_tfidf)\n",
        "\n",
        "for doc, category in zip(docs_new, predicted):\n",
        "     print('%r => %s' % (doc, twenty_train.target_names[category]))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_F2kTeqGu3Da",
        "outputId": "b44b204d-1e25-4884-9432-d092b4555587"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "'God is love' => soc.religion.christian\n",
            "'OpenGL on the GPU is fast' => comp.graphics\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Чтобы с цепочкой vectorizer => transformer => classifier было проще работать, в scikit-learn есть класс Pipeline, который функционирует как составной (конвейерный) классификатор:\n"
      ],
      "metadata": {
        "id": "wF8xzIPgAhex"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.pipeline import Pipeline\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                      ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', MultinomialNB()),\n",
        " ])"
      ],
      "metadata": {
        "id": "PbccM-iivAqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text_clf = text_clf.fit(twenty_train.data, twenty_train.target)"
      ],
      "metadata": {
        "id": "VwqaDRBkvIWZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Оценка точности прогноза модели"
      ],
      "metadata": {
        "id": "TwBJXEDLAqc4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "twenty_test = fetch_20newsgroups(subset='test',\n",
        "     categories=categories, shuffle=True, random_state=42)\n",
        "docs_test = twenty_test.data\n",
        "predicted = text_clf.predict(docs_test)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ELGZK9GnyJJV",
        "outputId": "2fa718f1-0a5d-4fc4-ae17-9f35ff94e5f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.8348868175765646"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "с помощью линейного метода опорных векторов (support vector machine (SVM))."
      ],
      "metadata": {
        "id": "oWJwlZkwA0N3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.linear_model import SGDClassifier\n",
        "text_clf = Pipeline([('vect', CountVectorizer()),\n",
        "                     ('tfidf', TfidfTransformer()),\n",
        "                      ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
        "                                            alpha=1e-3,  random_state=42)),\n",
        " ])\n",
        "_ = text_clf.fit(twenty_train.data, twenty_train.target)\n",
        "predicted = text_clf.predict(docs_test)\n",
        "np.mean(predicted == twenty_test.target)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N6SYklkdyWDM",
        "outputId": "959b3634-e3d0-4f7f-c227-7ed638ebcf4e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0.9114513981358189"
            ]
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn import metrics\n",
        "print(metrics.classification_report(twenty_test.target, predicted,\n",
        "     target_names=twenty_test.target_names))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lauM6ZfMy49z",
        "outputId": "45ffd07e-4cbd-44b0-e859-d4d9569150db"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                        precision    recall  f1-score   support\n",
            "\n",
            "           alt.atheism       0.95      0.81      0.88       319\n",
            "         comp.graphics       0.87      0.98      0.92       389\n",
            "               sci.med       0.96      0.88      0.92       396\n",
            "soc.religion.christian       0.89      0.96      0.92       398\n",
            "\n",
            "              accuracy                           0.91      1502\n",
            "             macro avg       0.92      0.91      0.91      1502\n",
            "          weighted avg       0.92      0.91      0.91      1502\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "metrics.confusion_matrix(twenty_test.target, predicted)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WGu6SNqT0N6S",
        "outputId": "11350484-f547-4e5a-9086-29fce8e8d03d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[259,  10,  12,  38],\n",
              "       [  3, 380,   2,   4],\n",
              "       [  6,  36, 349,   5],\n",
              "       [  5,  10,   2, 381]])"
            ]
          },
          "metadata": {},
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "l3ZJjokNBoll"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Некоторые параметры мы уже посчитали, такие как use_idf в функции TfidfTransformer.\n",
        "Вместо поиска параметров различных компонентов в цепи, можно запустить поиск (методом полного перебора ) лучших параметров в сетке возможных значений. Мы опробовали все классификаторы на словах или биграммах, с или без idf, с штрафными параметрами 0,01 и 0,001 для SVM (метода опорных векторов):\n",
        "\n"
      ],
      "metadata": {
        "id": "Wt6M2CbCBIzv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import sklearn\n",
        "from sklearn.model_selection import RandomizedSearchCV\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "parameters = {'vect__ngram_range': [(1, 1), (1, 2)],\n",
        "               'tfidf__use_idf': (True, False),\n",
        "               'clf__alpha': (1e-2, 1e-3),\n",
        " }\n",
        "\n"
      ],
      "metadata": {
        "id": "_jZC3CAW0cFQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "можем запустить grid search, чтобы попробовать все комбинации параметров параллельно с параметром n_jobs. Если мы зададим этому параметру значение -1, grid search определит, как много ядер установлено, и задействует их все:"
      ],
      "metadata": {
        "id": "ctJMMW5_BpnO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "gs_clf = GridSearchCV(text_clf, parameters, n_jobs=-1)\n",
        "\n",
        "gs_clf = gs_clf.fit(twenty_train.data[:400], twenty_train.target[:400])\n"
      ],
      "metadata": {
        "id": "1APs3YzgBk0m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx_pr = gs_clf.predict(['God is love'])\n",
        "twenty_train.target_names[idx_pr[0]]"
      ],
      "metadata": {
        "id": "O250mMFR5fM6",
        "outputId": "9e22acfe-741f-4d58-bd37-18ca64e25dc1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        }
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'soc.religion.christian'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    }
  ]
}